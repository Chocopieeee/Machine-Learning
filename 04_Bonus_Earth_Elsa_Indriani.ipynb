{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0WoJRd4sHbs_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan dataset MNIST\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "peyBqBVNHf5C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk melatih model\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images = images.view(-1, 28 * 28)  # Flatten the images\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")"
      ],
      "metadata": {
        "id": "h5rFF_QrH9_K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk menguji model\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.view(-1, 28 * 28)  # Flatten the images\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "TRJhs9sXIBNB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigurasi Fungsi Rugi 1: Cross-Entropy Loss\n",
        "criterion1 = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "sZ68HKxVIGRk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigurasi 1: Model Dangkal\n",
        "class ShallowNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ShallowNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Konfigurasi 2: Model Sedang\n",
        "class MediumNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MediumNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Konfigurasi 3: Model Dalam\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, 512)\n",
        "        self.fc4 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "P0eq4ieGIcVm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigurasi Fungsi Aktivasi 1: ReLU Activation\n",
        "model1 = ShallowNN()\n",
        "model2 = MediumNN()\n",
        "model3 = DeepNN()"
      ],
      "metadata": {
        "id": "Urz-9x5PISDC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size dan optimizer\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=learning_rate)\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)\n",
        "optimizer3 = optim.Adam(model3.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "XfR8XO8DIV0k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "7pL7kBWKIsFt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Latih dan uji model dengan fungsi aktivasi ReLU\n",
        "print(\"Konfigurasi 1: Model Dangkal dengan Fungsi Aktivasi ReLU\")\n",
        "train_model(model1, train_loader, criterion1, optimizer1)\n",
        "accuracy1 = test_model(model1, test_loader)\n",
        "print(f\"Akurasi: {accuracy1}%\")\n",
        "\n",
        "print(\"\\nKonfigurasi 2: Model Sedang dengan Fungsi Aktivasi ReLU\")\n",
        "train_model(model2, train_loader, criterion1, optimizer2)\n",
        "accuracy2 = test_model(model2, test_loader)\n",
        "print(f\"Akurasi: {accuracy2}%\")\n",
        "\n",
        "print(\"\\nKonfigurasi 3: Model Dalam dengan Fungsi Aktivasi ReLU \")\n",
        "train_model(model3, train_loader, criterion1, optimizer3)\n",
        "accuracy3 = test_model(model3, test_loader)\n",
        "print(f\"Akurasi: {accuracy3}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-4mIYI2IxeX",
        "outputId": "e3f8bf0a-52a3-490e-a0db-7bc936ebbc76"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Konfigurasi 1: Model Dangkal dengan Fungsi Aktivasi ReLU\n",
            "Epoch 1, Loss: 0.37130141707816355\n",
            "Epoch 2, Loss: 0.18931218582604611\n",
            "Epoch 3, Loss: 0.1382040367034802\n",
            "Epoch 4, Loss: 0.11088386652415305\n",
            "Epoch 5, Loss: 0.09311543940367904\n",
            "Akurasi: 96.52%\n",
            "\n",
            "Konfigurasi 2: Model Sedang dengan Fungsi Aktivasi ReLU\n",
            "Epoch 1, Loss: 0.32738004041823754\n",
            "Epoch 2, Loss: 0.14693075791497762\n",
            "Epoch 3, Loss: 0.10916783189329543\n",
            "Epoch 4, Loss: 0.08849660387840559\n",
            "Epoch 5, Loss: 0.07628323723460788\n",
            "Akurasi: 97.34%\n",
            "\n",
            "Konfigurasi 3: Model Dalam dengan Fungsi Aktivasi ReLU \n",
            "Epoch 1, Loss: 0.29791508445035675\n",
            "Epoch 2, Loss: 0.14638327584584068\n",
            "Epoch 3, Loss: 0.1096466313645458\n",
            "Epoch 4, Loss: 0.09243236990573048\n",
            "Epoch 5, Loss: 0.07977462199721128\n",
            "Akurasi: 97.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kesimpulan**"
      ],
      "metadata": {
        "id": "hsmSxrimYOIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penggunaan fungsi aktivasi ReLU pada lapisan-lapisan internal model adalah pilihan yang baik karena membantu menghindari masalah vanishing gradient dan memungkinkan model untuk mempelajari representasi yang lebih baik dari data. Hasil pelatihan menunjukkan bahwa model yang lebih dalam (konfigurasi 3) memiliki kinerja yang sedikit lebih baik daripada model dangkal (konfigurasi 1) dan model sedang (konfigurasi 2). Akurasi pengujian model cukup tinggi, dengan model dalam mencapai akurasi tertinggi sekitar 97.43%, yang menunjukkan bahwa model-model ini mampu mengenali digit tulisan tangan dengan baik. Selama pelatihan, loss (kerugian) yang digunakan adalah Cross-Entropy Loss. Loss berkurang seiring dengan berjalannya epoch, yang menunjukkan bahwa model-model ini berhasil belajar dari data pelatihan."
      ],
      "metadata": {
        "id": "_hw8O9KIYVFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigurasi 1: Model Dangkal dengan Fungsi Aktivasi Sigmoid pada Lapisan Output\n",
        "class ShallowNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ShallowNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Konfigurasi 2: Model Sedang dengan Fungsi Aktivasi Sigmoid pada Lapisan Output\n",
        "class MediumNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MediumNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.sigmoid = nn.Sigmoid()  # Tambahkan lapisan aktivasi Sigmoid\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.sigmoid(x)  # Gunakan aktivasi Sigmoid\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)  # Gunakan aktivasi Sigmoid\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Konfigurasi 3: Model Dalam dengan Fungsi Aktivasi Sigmoid pada Lapisan Output\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, 512)\n",
        "        self.fc4 = nn.Linear(512, 10)\n",
        "        self.sigmoid = nn.Sigmoid()  # Tambahkan lapisan aktivasi Sigmoid\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.sigmoid(x)  # Gunakan aktivasi Sigmoid\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)  # Gunakan aktivasi Sigmoid\n",
        "        x = self.fc3(x)\n",
        "        x = self.sigmoid(x)  # Gunakan aktivasi Sigmoid\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Jp_I6N61I09q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Konfigurasi Fungsi Aktivasi 2: Sigmoid Activation pada Lapisan Output\n",
        "model4 = ShallowNN()\n",
        "model4.fc1 = nn.Sequential(model4.fc1, nn.Sigmoid())\n",
        "model5 = MediumNN()\n",
        "model5.fc1 = nn.Sequential(model5.fc1, nn.Sigmoid())\n",
        "model6 = DeepNN()\n",
        "model6.fc1 = nn.Sequential(model6.fc1, nn.Sigmoid())"
      ],
      "metadata": {
        "id": "GSLBQxodKMe8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Latih dan uji model dengan fungsi aktivasi Sigmoid pada lapisan output\n",
        "print(\"Konfigurasi 1: Model Dangkal dengan Fungsi Aktivasi Sigmoid pada Lapisan Output\")\n",
        "train_model(model4, train_loader, criterion1, optimizer1)\n",
        "accuracy1 = test_model(model4, test_loader)\n",
        "print(f\"Akurasi: {accuracy1}%\")\n",
        "\n",
        "print(\"\\nKonfigurasi 2: Model Sedang dengan Fungsi Aktivasi Sigmoid pada Lapisan Output\")\n",
        "train_model(model5, train_loader, criterion1, optimizer2)\n",
        "accuracy2 = test_model(model5, test_loader)\n",
        "print(f\"Akurasi: {accuracy2}%\")\n",
        "\n",
        "print(\"\\nKonfigurasi 3: Model Dalam dengan Fungsi Aktivasi Sigmoid pada Lapisan Output\")\n",
        "train_model(model6, train_loader, criterion1, optimizer3)\n",
        "accuracy3 = test_model(model6, test_loader)\n",
        "print(f\"Akurasi: {accuracy3}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stTVD0lkKV9U",
        "outputId": "971d1816-0813-42f9-f84f-451ee1963634"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Konfigurasi 1: Model Dangkal dengan Fungsi Aktivasi Sigmoid pada Lapisan Output\n",
            "Epoch 1, Loss: 2.3564610765941105\n",
            "Epoch 2, Loss: 2.3564105430391553\n",
            "Epoch 3, Loss: 2.356473807078689\n",
            "Epoch 4, Loss: 2.3564385127411214\n",
            "Epoch 5, Loss: 2.356466654012961\n",
            "Akurasi: 8.97%\n",
            "\n",
            "Konfigurasi 2: Model Sedang dengan Fungsi Aktivasi Sigmoid pada Lapisan Output\n",
            "Epoch 1, Loss: 2.3564156426041367\n",
            "Epoch 2, Loss: 2.356375363081503\n",
            "Epoch 3, Loss: 2.356386170966793\n",
            "Epoch 4, Loss: 2.356380880006087\n",
            "Epoch 5, Loss: 2.3563704350863945\n",
            "Akurasi: 9.82%\n",
            "\n",
            "Konfigurasi 3: Model Dalam dengan Fungsi Aktivasi Sigmoid pada Lapisan Output\n",
            "Epoch 1, Loss: 2.375060582465963\n",
            "Epoch 2, Loss: 2.3750293758123924\n",
            "Epoch 3, Loss: 2.3750610570155226\n",
            "Epoch 4, Loss: 2.375085322587475\n",
            "Epoch 5, Loss: 2.375007926273956\n",
            "Akurasi: 9.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kesimpulan**"
      ],
      "metadata": {
        "id": "d3KkzmxUZCQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "kesimpulan utamanya adalah bahwa penggunaan fungsi aktivasi Sigmoid pada lapisan output kurang cocok untuk tugas klasifikasi gambar digit MNIST, dan ini menghasilkan performa yang sangat buruk dibandingkan dengan penggunaan aktivasi ReLU yang lebih umum digunakan dalam tugas semacam ini. Akurasi pengujian untuk ketiga konfigurasi model dengan aktivasi Sigmoid sangat rendah, dengan akurasi terbaik hanya sekitar 9.82% (konfigurasi 2). Ini menunjukkan bahwa model-model ini memiliki kesulitan yang signifikan dalam mengklasifikasikan digit tulisan tangan. Selama pelatihan, loss (kerugian) pada setiap epoch juga telah dicetak. Namun, nilai loss pada model-model ini tetap tinggi dan cenderung tidak berubah selama pelatihan. Penggunaan fungsi aktivasi Sigmoid pada lapisan output tidak cocok untuk tugas klasifikasi MNIST ini, dan ini adalah alasan utama mengapa performa model sangat buruk."
      ],
      "metadata": {
        "id": "8yDNYAWxZCWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F  # Import Fungsi Aktivasi\n",
        "\n",
        "# Konfigurasi 1: Model Dangkal dengan Fungsi Aktivasi Tanh pada Lapisan Output\n",
        "class ShallowNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ShallowNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = torch.tanh(x)  # Gunakan aktivasi Tanh\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Konfigurasi 2: Model Sedang dengan Fungsi Aktivasi Tanh pada Lapisan Output\n",
        "class MediumNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MediumNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = torch.tanh(x)  # Gunakan aktivasi Tanh\n",
        "        x = self.fc2(x)\n",
        "        x = torch.tanh(x)  # Gunakan aktivasi Tanh\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Konfigurasi 3: Model Dalam dengan Fungsi Aktivasi Tanh pada Lapisan Output\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, 512)\n",
        "        self.fc4 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = torch.tanh(x)  # Gunakan aktivasi Tanh\n",
        "        x = self.fc2(x)\n",
        "        x = torch.tanh(x)  # Gunakan aktivasi Tanh\n",
        "        x = self.fc3(x)\n",
        "        x = torch.tanh(x)  # Gunakan aktivasi Tanh\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "sWnqBtNhKjPf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigurasi Fungsi Aktivasi 3: Tanh Activation\n",
        "model7 = ShallowNN()\n",
        "model7.fc1 = nn.Sequential(model7.fc1, nn.Tanh())\n",
        "model8 = MediumNN()\n",
        "model8.fc1 = nn.Sequential(model8.fc1, nn.Tanh())\n",
        "model9 = DeepNN()\n",
        "model9.fc1 = nn.Sequential(model9.fc1, nn.Tanh())"
      ],
      "metadata": {
        "id": "yuniAd_GS5yR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Latih dan uji model dengan fungsi aktivasi Tanh\n",
        "print(\"Konfigurasi 1: Model Dangkal dengan Fungsi Aktivasi Tanh\")\n",
        "train_model(model7, train_loader, criterion1, optimizer1)\n",
        "accuracy1 = test_model(model7, test_loader)\n",
        "print(f\"Akurasi: {accuracy1}%\")\n",
        "\n",
        "print(\"\\nKonfigurasi 2: Model Sedang dengan Fungsi Aktivasi Tanh\")\n",
        "train_model(model8, train_loader, criterion1, optimizer2)\n",
        "accuracy2 = test_model(model8, test_loader)\n",
        "print(f\"Akurasi: {accuracy2}%\")\n",
        "\n",
        "print(\"\\nKonfigurasi 3: Model Dalam dengan Fungsi Aktivasi Tanh\")\n",
        "train_model(model9, train_loader, criterion1, optimizer3)\n",
        "accuracy3 = test_model(model9, test_loader)\n",
        "print(f\"Akurasi: {accuracy3}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik6EuJ4zTDVF",
        "outputId": "db93da9a-ca67-47f1-85f3-0c4f5e173f3d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Konfigurasi 1: Model Dangkal dengan Fungsi Aktivasi Tanh\n",
            "Epoch 1, Loss: 2.3454594162227247\n",
            "Epoch 2, Loss: 2.3454364367893765\n",
            "Epoch 3, Loss: 2.3454128239454746\n",
            "Epoch 5, Loss: 2.3454474762304507\n",
            "Akurasi: 9.56%\n",
            "\n",
            "Konfigurasi 2: Model Sedang dengan Fungsi Aktivasi Tanh\n",
            "Epoch 1, Loss: 2.2900415918211947\n",
            "Epoch 2, Loss: 2.2900352472943792\n",
            "Epoch 3, Loss: 2.2900214152041274\n",
            "Epoch 4, Loss: 2.290051623193948\n",
            "Epoch 5, Loss: 2.2900432284706946\n",
            "Akurasi: 11.51%\n",
            "\n",
            "Konfigurasi 3: Model Dalam dengan Fungsi Aktivasi Tanh\n",
            "Epoch 1, Loss: 2.304262038995462\n",
            "Epoch 2, Loss: 2.304267259294799\n",
            "Epoch 3, Loss: 2.304275230558188\n",
            "Epoch 4, Loss: 2.3042679437950477\n",
            "Epoch 5, Loss: 2.304276392149773\n",
            "Akurasi: 10.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kesimpulan**"
      ],
      "metadata": {
        "id": "KWE3WuZtahmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil pelatihan menunjukkan bahwa model dengan fungsi aktivasi Tanh pada lapisan output juga menghasilkan performa yang buruk, meskipun sedikit lebih baik daripada model dengan fungsi aktivasi Sigmoid. Akurasi pengujian untuk ketiga konfigurasi model dengan aktivasi Tanh masih sangat rendah, dengan akurasi terbaik sekitar 11.51% (konfigurasi 2). Ini menunjukkan bahwa model-model ini masih mengalami kesulitan yang signifikan dalam mengklasifikasikan digit tulisan tangan. Selama pelatihan, loss (kerugian) pada setiap epoch juga telah dicetak. Namun, nilai loss pada model-model ini tetap tinggi dan cenderung tidak berubah selama pelatihan. Penggunaan fungsi aktivasi Tanh pada lapisan output juga kurang cocok untuk tugas klasifikasi gambar digit MNIST ini. Fungsi aktivasi Tanh biasanya lebih cocok untuk tugas regresi, sedangkan untuk tugas klasifikasi, fungsi aktivasi ReLU atau Softmax lebih umum digunakan."
      ],
      "metadata": {
        "id": "nTs8wqEtahu5"
      }
    }
  ]
}